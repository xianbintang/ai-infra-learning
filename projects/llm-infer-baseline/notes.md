# å®éªŒè®°å½•æ¨¡æ¿

å¤åˆ¶è¯¥æ¨¡æ¿è®°å½•æ¯æ¬¡å®éªŒï¼Œç¡®ä¿èƒ½å¤Ÿå¤ç°ã€‚

```
## Experiment: <name>
Date: <YYYY-MM-DD>
Hardware: <GPU / CPU å‹å·>
Model: distilgpt2 @ <dtype>
Prompt: "..."
Batch policy: <size / ç­‰å¾…çª—å£>
KV Cache: on/off

Metrics
-------
load_model        :
load_tokenizer    :
prefill           :
decode_loop       :
token/s           :
GPU Utilization   :
Notes             :
```

---

## Experiment: day1-baseline-setup
Date: 2024-12-19
Hardware: Apple Silicon (CPU inference, no dedicated GPU, arm64)
Model: distilgpt2 @ torch.float32
Prompt: "Q: Explain why batching improves GPU utilization.\nA:"
Batch policy: single request (no batching)
KV Cache: on (use_cache=True)

Metrics
-------
load_model+tok    : 98.30 s (é¦–æ¬¡åŠ è½½ï¼Œå«ç½‘ç»œé‡è¯•)
tokenize          : 2.51 ms (13 tokens)
prefill           : 434.26 ms (13 tokens, 33.40 ms/token)
decode_loop       : 40.70 ms (9 steps, 4.52 ms/step avg)
total_generate    : 474.96 ms
token/s           : 21.05 tokens/s
GPU Utilization   : N/A (CPU only)

Notes:
- ç¯å¢ƒ: macOS arm64, torch.float32, CPU
- Prompt é•¿åº¦: 13 tokens
- ç”Ÿæˆé•¿åº¦: 10 tokens (å«é¦–ä¸ª token)
- ç”Ÿæˆæ–‡æœ¬: "The GPU is a very powerful GPU, and it"
- KV Cache æœ€ç»ˆ shape: (1, 12, 22, 64) = [batch, heads, seq_len=13+9, head_dim]

å…³é”®å‘ç°:
- Prefill æ¯ token (33.40 ms) è¿œæ…¢äº Decode æ¯ step (4.52 ms)
  - åŸå› : Prefill éœ€è¦å¯¹æ‰€æœ‰è¾“å…¥ token åšå®Œæ•´çš„ attention è®¡ç®—
  - Decode æœ‰ KV Cache åŠ é€Ÿï¼Œåªéœ€å¤„ç† 1 ä¸ªæ–° token
- Decode æ¯æ­¥è€—æ—¶ç¨³å®šåœ¨ 4-5 msï¼Œè¯´æ˜ KV Cache æœ‰æ•ˆå¤ç”¨
- æ¨¡å‹åŠ è½½è€—æ—¶é•¿æ˜¯å› ä¸º HuggingFace ç½‘ç»œé‡è¯• (ConnectionResetError)

---

## Experiment: day1-prefill-decode-deep-dive
Date: 2024-12-19
Hardware: Apple Silicon (CPU inference, arm64)
Model: distilgpt2 @ torch.float32
Script: inspect_prefill_decode.py

ç›®çš„: æ‰‹åŠ¨æ‹†è§£ Prefill å’Œ Decode é˜¶æ®µï¼Œè§‚å¯Ÿ KV Cache å˜åŒ–

Prefill é˜¶æ®µ:
- è¾“å…¥: 13 tokens
- è¾“å‡º: logits (1, 13, 50257), KV Cache 6 å±‚ Ã— (1, 12, 13, 64)
- é¦–ä¸ª token é€‰æ‹©: argmax(logits[0, -1]) -> ID 383 " The"

Decode é˜¶æ®µ (æ¯æ­¥):
- è¾“å…¥: 1 token + past_key_values
- è¾“å‡º: logits (1, 1, 50257), æ›´æ–°åçš„ KV Cache
- KV Cache seq_len: 13 -> 14 -> 15 -> ... -> 22

Decode å„æ­¥è€—æ—¶ (ms):
| Step | è€—æ—¶ | KV seq_len | é€‰ä¸­ token |
|------|------|------------|------------|
| 1    | 5.25 | 14         | " GPU"     |
| 2    | 4.85 | 15         | " is"      |
| 3    | 4.14 | 16         | " a"       |
| 4    | 4.68 | 17         | " very"    |
| 5    | 4.34 | 18         | " powerful"|
| 6    | 4.60 | 19         | " GPU"     |
| 7    | 4.45 | 20         | ","        |
| 8    | 4.36 | 21         | " and"     |
| 9    | 4.03 | 22         | " it"      |

ç»“è®º:
- æ‰‹åŠ¨å®ç° generate() éªŒè¯äº† Prefill + Decode çš„æ ¸å¿ƒé€»è¾‘
- KV Cache æ˜¯åŠ é€Ÿ Decode çš„å…³é”®ï¼Œseq_len çº¿æ€§å¢é•¿
- æ¯æ­¥ Decode è€—æ—¶ç¨³å®šï¼Œä¸éš seq_len æ˜¾è‘—å¢é•¿ (åœ¨å°è§„æ¨¡ä¸‹)
- ç†è§£è¿™äº›æœºåˆ¶æ˜¯ä¼˜åŒ– LLM æ¨ç†çš„åŸºç¡€

---

## Experiment: day2-batching-tradeoff
Date: 2025-12-19
Hardware: Apple Silicon (CPU inference, arm64)
Model: distilgpt2 @ torch.float32
Script: batch_experiment.py
Total Requests: 16
Max New Tokens: 32

### å®éªŒæ•°æ®ï¼ˆæœ€æ–°è¿è¡Œï¼Œæ¨¡å‹å·²ç¼“å­˜ï¼‰

| Batch | Wait(ms) | Total(s) | Throughput | Avg Lat | P95 Lat |
|-------|----------|----------|------------|---------|---------|
| 1 | 0 | 2.509 | 204.0 t/s | 156.7 ms | 558.8 ms |
| 1 | 10 | 2.263 | 226.2 t/s | 141.4 ms | 161.7 ms |
| 4 | 0 | 1.269 | 414.4 t/s | 317.2 ms | 331.1 ms |
| 4 | 10 | 1.271 | 413.9 t/s | 317.6 ms | 328.0 ms |
| 8 | 0 | 0.614 | **856.9 t/s** | **306.7 ms** | 309.3 ms |
| 8 | 10 | 0.747 | 703.7 t/s | 373.5 ms | 396.9 ms |

### Trade-off åˆ†æ

ç›¸å¯¹äº Batch=1 (wait=0ms):
- **Batch=4**: ååé‡ +103.1%, å»¶è¿Ÿ +102.4%
- **Batch=8**: ååé‡ **+320.0%**, å»¶è¿Ÿ **+95.8%**

å…³é”®å‘ç°:
```
Batch=1: 204 t/s, 157ms   â† ä½å»¶è¿Ÿï¼Œä½åå
Batch=4: 414 t/s, 317ms   â† 2x ååï¼Œ2x å»¶è¿Ÿ
Batch=8: 857 t/s, 307ms   â† 4x ååï¼Œå»¶è¿Ÿåè€Œæ›´ä½ï¼ğŸ‰
```

### æƒŠå–œå‘ç°ï¼šBatch=8 å»¶è¿Ÿæ¯” Batch=4 è¿˜ä½ï¼

**ç°è±¡**: Batch=8 (307ms) < Batch=4 (317ms)

**åŸå› åˆ†æ**:

1. **å‡å°‘äº†æ‰¹æ¬¡é—´å¼€é”€**
   - Batch=1 éœ€è¦ 16 æ¬¡ç‹¬ç«‹è°ƒç”¨ï¼Œæ¯æ¬¡æœ‰ Python/æ¡†æ¶å¼€é”€
   - Batch=4 éœ€è¦ 4 æ¬¡è°ƒç”¨
   - Batch=8 åªéœ€è¦ 2 æ¬¡è°ƒç”¨
   - æ›´å°‘çš„è°ƒç”¨ = æ›´å°‘çš„è°ƒåº¦å¼€é”€

2. **CPU ç¼“å­˜å‹å¥½æ€§**
   - å¤§ batch æ—¶ï¼Œæƒé‡çŸ©é˜µåœ¨ L2/L3 ç¼“å­˜ä¸­å¤ç”¨æ›´å……åˆ†
   - å° batch åå¤åŠ è½½æƒé‡ï¼Œç¼“å­˜å‘½ä¸­ç‡ä½

3. **SIMD å‘é‡åŒ–æ•ˆç‡**
   - Apple Silicon çš„ NEON æŒ‡ä»¤é›†åœ¨å¤„ç†è¿ç»­æ•°æ®æ—¶æ•ˆç‡æ›´é«˜
   - Batch=8 è®©çŸ©é˜µè¿ç®—æ›´å¥½åœ°åˆ©ç”¨ SIMD å®½åº¦

4. **å†…å­˜å¸¦å®½æ‘Šè–„**
   - æ¨¡å‹æƒé‡è¯»å–æ˜¯å›ºå®šæˆæœ¬
   - å¤§ batch å°†è¿™ä¸ªæˆæœ¬åˆ†æ‘Šåˆ°æ›´å¤šè¯·æ±‚ä¸Š

### ç»“è®º

1. **ç‰ºç‰²è°æ¢è°**: Batching ç”¨å»¶è¿Ÿæ¢ååé‡ (ä½†æœ‰æƒŠå–œï¼)
   - é€šå¸¸ï¼šæ›´å¤§çš„ batch = æ›´é«˜çš„åå + æ›´é«˜çš„å»¶è¿Ÿ
   - **å®é™…**ï¼šBatch=8 åŒæ—¶è·å¾—äº†æœ€é«˜ååå’Œè¾ƒä½å»¶è¿Ÿ
   - æ•ˆç‡æ¯” = 320% / 96% â‰ˆ **3.3x** ğŸš€

2. **æœ€ä½³å®è·µ**:
   - ä¸è¦å‡è®¾å¤§ batch ä¸€å®šé«˜å»¶è¿Ÿ
   - éœ€è¦å®æµ‹æ‰¾åˆ°ç¡¬ä»¶çš„"ç”œç‚¹" batch size
   - Apple Silicon CPU åœ¨ batch=8 é™„è¿‘æ•ˆç‡æœ€ä¼˜

3. **ä¸šåŠ¡åœºæ™¯é€‰æ‹©**:
   - å®æ—¶å¯¹è¯: batch=1-2 (ä½å»¶è¿Ÿä¼˜å…ˆ)
   - API æœåŠ¡: batch=4-8 (å¹³è¡¡ï¼Œå®æµ‹è°ƒä¼˜)
   - æ‰¹é‡å¤„ç†: batch=8+ (é«˜ååä¼˜å…ˆ)

4. **ç­‰å¾…çª—å£å½±å“**:
   - wait=10ms å¢åŠ äº†çº¦ 12ms æ’é˜Ÿæ—¶é—´
   - Batch=8 + wait=10ms ååä¸‹é™åˆ° 704 t/s
   - ç»“è®ºï¼šåœ¨è¯·æ±‚å……è¶³æ—¶ï¼Œwait=0 æ•ˆæœæ›´å¥½

5. **CPU vs GPU é¢„æœŸå·®å¼‚**:
   - CPU æ¨ç†å·²ç»çœ‹åˆ° 4x ååæå‡
   - GPU æ¨ç†ä¸‹ batching æ”¶ç›Šä¼šæ›´æ˜æ˜¾ï¼ˆå¹¶è¡Œè®¡ç®—æ›´å¼ºï¼‰
   - æœŸå¾…åœ¨ GPU ç¯å¢ƒä¸‹çœ‹åˆ°æ›´å¤§çš„ååæå‡
