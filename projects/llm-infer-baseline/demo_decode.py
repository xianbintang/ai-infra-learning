"""æ¼”ç¤º tokenizer.decode() çš„è¿‡ç¨‹å’Œé€»è¾‘ã€‚"""
from __future__ import annotations

import torch
from transformers import AutoTokenizer

from config import MODEL_NAME


def demo_basic_decode() -> None:
    """æ¼”ç¤ºåŸºæœ¬çš„ decode è¿‡ç¨‹ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ“ æ¼”ç¤º 1: åŸºæœ¬ decode è¿‡ç¨‹")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç¼–ç 
    text = "Hello world!"
    print(f"\n1. åŸå§‹æ–‡æœ¬: {text}")
    
    token_ids = tokenizer.encode(text)
    print(f"2. ç¼–ç åçš„ Token IDs: {token_ids}")
    
    # æ˜¾ç¤ºæ¯ä¸ª token çš„è¯¦ç»†ä¿¡æ¯
    print(f"\n3. Token è¯¦ç»†ä¿¡æ¯:")
    for i, token_id in enumerate(token_ids):
        token = tokenizer.convert_ids_to_tokens([token_id])[0]
        print(f"   [{i}] ID: {token_id:5d} â†’ Token: '{token}'")
    
    # è§£ç 
    decoded = tokenizer.decode(token_ids)
    print(f"\n4. è§£ç åçš„æ–‡æœ¬: '{decoded}'")
    print(f"5. æ˜¯å¦å®Œå…¨åŒ¹é…: {text == decoded}")


def demo_decode_with_special_tokens() -> None:
    """æ¼”ç¤ºç‰¹æ®Š token çš„å¤„ç†ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ”– æ¼”ç¤º 2: ç‰¹æ®Š Token å¤„ç†")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŒ…å«ç‰¹æ®Š token çš„åºåˆ—
    text = "Hello world!"
    token_ids = tokenizer.encode(text)
    
    # æ·»åŠ  EOS token
    eos_id = tokenizer.eos_token_id
    token_ids_with_eos = token_ids + [eos_id]
    
    print(f"\n1. åŸå§‹ Token IDs: {token_ids}")
    print(f"2. æ·»åŠ  EOS token: {token_ids_with_eos}")
    print(f"   EOS token ID: {eos_id}")
    print(f"   EOS token å­—ç¬¦ä¸²: '{tokenizer.eos_token}'")
    
    # ä¸è·³è¿‡ç‰¹æ®Š token
    decoded_with_special = tokenizer.decode(
        token_ids_with_eos,
        skip_special_tokens=False
    )
    print(f"\n3. è§£ç ï¼ˆä¿ç•™ç‰¹æ®Š tokenï¼‰: '{decoded_with_special}'")
    
    # è·³è¿‡ç‰¹æ®Š token
    decoded_without_special = tokenizer.decode(
        token_ids_with_eos,
        skip_special_tokens=True
    )
    print(f"4. è§£ç ï¼ˆè·³è¿‡ç‰¹æ®Š tokenï¼‰: '{decoded_without_special}'")
    
    print(f"\nğŸ’¡ åŒºåˆ«: æ˜¯å¦åŒ…å« '{tokenizer.eos_token}' æ ‡è®°")


def demo_decode_process() -> None:
    """æ¼”ç¤º decode çš„è¯¦ç»†è¿‡ç¨‹ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ” æ¼”ç¤º 3: decode è¯¦ç»†è¿‡ç¨‹")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç¤ºä¾‹ token IDs
    token_ids = [15496, 995, 33]  # "Hello world!"
    
    print(f"\nè¾“å…¥ Token IDs: {token_ids}")
    
    # æ­¥éª¤ 1: æŸ¥æ‰¾è¯æ±‡è¡¨
    print(f"\næ­¥éª¤ 1: æŸ¥æ‰¾è¯æ±‡è¡¨ï¼ˆID â†’ Token å­—ç¬¦ä¸²ï¼‰")
    tokens = []
    for token_id in token_ids:
        token = tokenizer.convert_ids_to_tokens([token_id])[0]
        tokens.append(token)
        print(f"   ID {token_id:5d} â†’ '{token}'")
    
    # æ­¥éª¤ 2: æ˜¾ç¤º BPE å¤„ç†
    print(f"\næ­¥éª¤ 2: BPE Token åˆ†æ")
    for i, token in enumerate(tokens):
        if token.startswith("Ä "):
            print(f"   [{i}] '{token}' â†’ å‰é¢æœ‰ç©ºæ ¼ï¼ˆÄ  è¡¨ç¤ºç©ºæ ¼ï¼‰")
            print(f"       å¤„ç†: æ·»åŠ ç©ºæ ¼ + '{token[1:]}'")
        else:
            print(f"   [{i}] '{token}' â†’ ç›´æ¥æ·»åŠ ")
    
    # æ­¥éª¤ 3: åˆå¹¶ç»“æœ
    print(f"\næ­¥éª¤ 3: åˆå¹¶ BPE tokens")
    text = tokenizer.decode(token_ids)
    print(f"   ç»“æœ: '{text}'")
    
    # æ­¥éª¤ 4: éªŒè¯
    print(f"\næ­¥éª¤ 4: éªŒè¯")
    print(f"   åŸå§‹ Token IDs: {token_ids}")
    print(f"   è§£ç åæ–‡æœ¬: '{text}'")
    print(f"   å¯ä»¥é‡æ–°ç¼–ç : {tokenizer.encode(text) == token_ids}")


def demo_decode_from_model_output() -> None:
    """æ¼”ç¤ºå¦‚ä½•å¤„ç†æ¨¡å‹è¾“å‡ºã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ¤– æ¼”ç¤º 4: å¤„ç†æ¨¡å‹è¾“å‡º")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºï¼ˆåŒ…å« prompt + ç”Ÿæˆéƒ¨åˆ†ï¼‰
    prompt = "Hello"
    prompt_ids = tokenizer.encode(prompt)
    
    # æ¨¡æ‹Ÿç”Ÿæˆçš„éƒ¨åˆ†
    generated_ids = [995, 33]  # " world!"
    
    # å®Œæ•´è¾“å‡º
    full_output = prompt_ids + generated_ids
    
    print(f"\n1. Prompt: '{prompt}'")
    print(f"   Prompt Token IDs: {prompt_ids}")
    print(f"\n2. ç”Ÿæˆéƒ¨åˆ† Token IDs: {generated_ids}")
    print(f"\n3. å®Œæ•´è¾“å‡º Token IDs: {full_output}")
    
    # è§£ç å®Œæ•´è¾“å‡º
    full_text = tokenizer.decode(full_output, skip_special_tokens=True)
    print(f"\n4. å®Œæ•´æ–‡æœ¬: '{full_text}'")
    
    # åªè§£ç ç”Ÿæˆéƒ¨åˆ†
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    print(f"5. ä»…ç”Ÿæˆéƒ¨åˆ†: '{generated_text}'")
    
    # å¯¹æ¯”
    print(f"\nğŸ’¡ åŒºåˆ«:")
    print(f"   å®Œæ•´æ–‡æœ¬åŒ…å« prompt: '{full_text}'")
    print(f"   ä»…ç”Ÿæˆéƒ¨åˆ†: '{generated_text}'")


def demo_decode_performance() -> None:
    """æ¼”ç¤º decode çš„æ€§èƒ½ã€‚"""
    print("\n" + "=" * 60)
    print("âš¡ æ¼”ç¤º 5: decode æ€§èƒ½")
    print("=" * 60)
    
    import time
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç”Ÿæˆä¸åŒé•¿åº¦çš„ token åºåˆ—
    test_cases = [
        ("çŸ­åºåˆ—", [15496, 995, 33]),
        ("ä¸­ç­‰åºåˆ—", list(range(100))),
        ("é•¿åºåˆ—", list(range(1000))),
    ]
    
    print(f"\næ€§èƒ½æµ‹è¯•:")
    for name, token_ids in test_cases:
        # å•ä¸ª decode
        start = time.perf_counter()
        text = tokenizer.decode(token_ids, skip_special_tokens=True)
        single_time = time.perf_counter() - start
        
        # æ‰¹é‡ decodeï¼ˆæ¨¡æ‹Ÿï¼‰
        start = time.perf_counter()
        for _ in range(100):
            _ = tokenizer.decode(token_ids, skip_special_tokens=True)
        batch_time = time.perf_counter() - start
        
        print(f"\n{name} ({len(token_ids)} tokens):")
        print(f"  å•æ¬¡ decode: {single_time*1000:.3f} ms")
        print(f"  100æ¬¡ decode: {batch_time*1000:.3f} ms")
        print(f"  å¹³å‡æ¯æ¬¡: {batch_time/100*1000:.3f} ms")


def demo_decode_edge_cases() -> None:
    """æ¼”ç¤º decode çš„è¾¹ç•Œæƒ…å†µã€‚"""
    print("\n" + "=" * 60)
    print("âš ï¸  æ¼”ç¤º 6: è¾¹ç•Œæƒ…å†µ")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æƒ…å†µ 1: ç©ºåºåˆ—
    print("\n1. ç©ºåºåˆ—:")
    empty_ids = []
    empty_text = tokenizer.decode(empty_ids)
    print(f"   Token IDs: {empty_ids}")
    print(f"   è§£ç ç»“æœ: '{empty_text}'")
    
    # æƒ…å†µ 2: åªæœ‰ç‰¹æ®Š token
    print("\n2. åªæœ‰ç‰¹æ®Š token:")
    eos_only = [tokenizer.eos_token_id]
    eos_text_with = tokenizer.decode(eos_only, skip_special_tokens=False)
    eos_text_without = tokenizer.decode(eos_only, skip_special_tokens=True)
    print(f"   Token IDs: {eos_only}")
    print(f"   ä¿ç•™ç‰¹æ®Š token: '{eos_text_with}'")
    print(f"   è·³è¿‡ç‰¹æ®Š token: '{eos_text_without}'")
    
    # æƒ…å†µ 3: Tensor è¾“å…¥
    print("\n3. Tensor è¾“å…¥:")
    tensor_ids = torch.tensor([15496, 995, 33])
    tensor_text = tokenizer.decode(tensor_ids)
    print(f"   è¾“å…¥ç±»å‹: {type(tensor_ids)}")
    print(f"   è§£ç ç»“æœ: '{tensor_text}'")
    
    # æƒ…å†µ 4: æ— æ•ˆ token ID
    print("\n4. æ— æ•ˆ token ID:")
    invalid_ids = [999999]  # ä¸å­˜åœ¨çš„ ID
    try:
        invalid_text = tokenizer.decode(invalid_ids)
        print(f"   Token IDs: {invalid_ids}")
        print(f"   è§£ç ç»“æœ: '{invalid_text}'")
        print(f"   ğŸ’¡ tokenizer ä¼šä½¿ç”¨ unk_token æˆ–è·³è¿‡")
    except Exception as e:
        print(f"   é”™è¯¯: {e}")


def demo_encode_decode_roundtrip() -> None:
    """æ¼”ç¤ºç¼–ç -è§£ç çš„å¾€è¿”è¿‡ç¨‹ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æ¼”ç¤º 7: ç¼–ç -è§£ç å¾€è¿”")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    test_texts = [
        "Hello world!",
        "The quick brown fox jumps over the lazy dog.",
        "Python is a programming language.",
        "   Multiple   spaces   here   ",
    ]
    
    print(f"\næµ‹è¯•æ–‡æœ¬çš„ç¼–ç -è§£ç å¾€è¿”:")
    for text in test_texts:
        # ç¼–ç 
        token_ids = tokenizer.encode(text)
        
        # è§£ç 
        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
        
        # æ¯”è¾ƒ
        match = text.strip() == decoded.strip()
        
        print(f"\nåŸæ–‡: '{text}'")
        print(f"  Token IDs: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}")
        print(f"  è§£ç : '{decoded}'")
        print(f"  åŒ¹é…: {'âœ…' if match else 'âŒ'}")
        if not match:
            print(f"  å·®å¼‚: åŸå§‹é•¿åº¦={len(text)}, è§£ç é•¿åº¦={len(decoded)}")


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸ”¬ tokenizer.decode() è¯¦ç»†æ¼”ç¤º")
    print("=" * 60)
    print(f"\næ¨¡å‹: {MODEL_NAME}")
    
    # æ¼”ç¤º 1: åŸºæœ¬ decode
    demo_basic_decode()
    
    # æ¼”ç¤º 2: ç‰¹æ®Š token å¤„ç†
    demo_decode_with_special_tokens()
    
    # æ¼”ç¤º 3: è¯¦ç»†è¿‡ç¨‹
    demo_decode_process()
    
    # æ¼”ç¤º 4: æ¨¡å‹è¾“å‡ºå¤„ç†
    demo_decode_from_model_output()
    
    # æ¼”ç¤º 5: æ€§èƒ½
    demo_decode_performance()
    
    # æ¼”ç¤º 6: è¾¹ç•Œæƒ…å†µ
    demo_decode_edge_cases()
    
    # æ¼”ç¤º 7: å¾€è¿”æµ‹è¯•
    demo_encode_decode_roundtrip()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 60)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("   1. decode å°† Token IDs è½¬æ¢ä¸ºæ–‡æœ¬")
    print("   2. éœ€è¦æŸ¥æ‰¾è¯æ±‡è¡¨å’Œåˆå¹¶ BPE tokens")
    print("   3. å¯ä»¥è·³è¿‡ç‰¹æ®Š token ä»¥å¾—åˆ°å¹²å‡€è¾“å‡º")
    print("   4. æ€§èƒ½å¾ˆå¥½ï¼Œé€šå¸¸ < 1ms")
    print("   5. ç¼–ç -è§£ç ä¸å®Œå…¨å¯é€†ï¼ˆå¯èƒ½ä¸¢å¤±ç©ºæ ¼ç­‰ï¼‰")


if __name__ == "__main__":
    main()


