"""è¯¦ç»†è§‚å¯Ÿ Prefill å’Œ Decode è¿‡ç¨‹çš„è„šæœ¬ï¼ˆå«è€—æ—¶æµ‹é‡ï¼‰ã€‚"""
import time
import torch
from config import MODEL_NAME, DEVICE, describe_environment
from model import load_model
from tokenizer import load_tokenizer

PROMPT = "Q: Explain why batching improves GPU utilization.\nA:"
MAX_NEW_TOKENS = 10  # å¢åŠ åˆ° 10 ä¸ªï¼Œä¾¿äºè§‚å¯Ÿ

def topk_tokens(tokenizer, logits_1d, k=5):
    """è¿”å› top-k å€™é€‰ tokenã€‚"""
    vals, idx = torch.topk(logits_1d, k)
    items = []
    for v, i in zip(vals.tolist(), idx.tolist()):
        tok = tokenizer.decode([i])
        items.append((i, tok.replace("\n", "\\n"), v))
    return items

def main():
    print("=" * 60)
    print("Prefill vs Decode è¯¦ç»†è§‚å¯Ÿï¼ˆå«è€—æ—¶æµ‹é‡ï¼‰")
    print("=" * 60)
    
    # ---------- ç¯å¢ƒä¿¡æ¯ ----------
    print("\n[ENVIRONMENT]")
    print("Environment:", describe_environment())
    print("Model:", MODEL_NAME)
    print("Device:", DEVICE)
    
    # ---------- åŠ è½½æ¨¡å‹ ----------
    print("\n[LOADING]")
    load_start = time.perf_counter()
    model = load_model(MODEL_NAME)
    tok = load_tokenizer(MODEL_NAME)
    load_time = time.perf_counter() - load_start
    print(f"Model + Tokenizer åŠ è½½è€—æ—¶: {load_time:.4f} s")
    
    # ---------- Tokenize ----------
    print("\n[INPUT]")
    tokenize_start = time.perf_counter()
    inputs = tok(PROMPT, return_tensors="pt").to(DEVICE)
    tokenize_time = time.perf_counter() - tokenize_start
    
    input_ids = inputs["input_ids"]
    prompt_len = input_ids.shape[-1]
    print(f"Prompt: {repr(PROMPT)}")
    print(f"Prompt token æ•°: {prompt_len}")
    print(f"input_ids.shape: {tuple(input_ids.shape)}")
    print(f"Tokenize è€—æ—¶: {tokenize_time*1000:.4f} ms")

    # ---------- Prefill ----------
    print("\n" + "=" * 60)
    print("[PREFILL PHASE]")
    print("=" * 60)
    
    prefill_start = time.perf_counter()
    with torch.no_grad():
        out = model(input_ids=input_ids, use_cache=True)
    prefill_time = time.perf_counter() - prefill_start

    logits = out.logits
    pkv = out.past_key_values

    print(f"\nPrefill è€—æ—¶: {prefill_time*1000:.4f} ms")
    print(f"Prefill å¤„ç† token æ•°: {prompt_len}")
    print(f"Prefill æ¯ token è€—æ—¶: {prefill_time*1000/prompt_len:.4f} ms/token")
    
    print(f"\nlogits.shape: {tuple(logits.shape)} = [batch, seq_len, vocab]")
    print(f"past_key_values å±‚æ•°: {len(pkv)}")
    k0, v0 = pkv[0]
    print(f"layer0 K.shape: {tuple(k0.shape)} = [batch, heads, seq_len, head_dim]")
    print(f"layer0 V.shape: {tuple(v0.shape)}")

    # é€‰æ‹©ç¬¬ä¸€ä¸ªç”Ÿæˆçš„ token
    next_logits = logits[0, -1]
    print(f"\n[Prefill -> é€‰æ‹©ä¸‹ä¸€ä¸ª token]")
    print(f"top-5 å€™é€‰:")
    for i, t, v in topk_tokens(tok, next_logits, k=5):
        print(f"  ID {i:6d}  {t!r:12s}  logit={v:.4f}")

    next_id = torch.argmax(next_logits).view(1, 1)
    print(f"é€‰ä¸­: ID {next_id.item()} -> {repr(tok.decode(next_id[0].tolist()))}")

    # ---------- Decode Loop ----------
    print("\n" + "=" * 60)
    print("[DECODE PHASE]")
    print("=" * 60)
    
    generated = [next_id]
    past = pkv
    decode_times = []

    for step in range(1, MAX_NEW_TOKENS):
        decode_start = time.perf_counter()
        with torch.no_grad():
            out = model(input_ids=next_id, past_key_values=past, use_cache=True)
        decode_time = time.perf_counter() - decode_start
        decode_times.append(decode_time)

        logits = out.logits
        past = out.past_key_values
        next_logits = logits[0, -1]

        k0, v0 = past[0]
        current_seq_len = k0.shape[2]
        
        print(f"\n[Decode Step {step}]")
        print(f"  è¾“å…¥ token: ID {next_id.item()} -> {repr(tok.decode([next_id.item()]))}")
        print(f"  è€—æ—¶: {decode_time*1000:.4f} ms")
        print(f"  KV Cache seq_len: {current_seq_len} (å¢é•¿äº† 1)")
        print(f"  top-3 å€™é€‰:")
        for i, t, v in topk_tokens(tok, next_logits, k=3):
            print(f"    ID {i:6d}  {t!r:12s}  logit={v:.4f}")

        next_id = torch.argmax(next_logits).view(1, 1)
        generated.append(next_id)
        print(f"  é€‰ä¸­: ID {next_id.item()} -> {repr(tok.decode(next_id[0].tolist()))}")

    # ---------- ç”Ÿæˆç»“æœ ----------
    gen_ids = torch.cat(generated, dim=-1)[0].tolist()
    generated_text = tok.decode(gen_ids, skip_special_tokens=True)
    
    print("\n" + "=" * 60)
    print("[GENERATED RESULT]")
    print("=" * 60)
    print(f"ç”Ÿæˆçš„ token IDs: {gen_ids}")
    print(f"ç”Ÿæˆçš„æ–‡æœ¬: {repr(generated_text)}")

    # ---------- æ€§èƒ½ç»Ÿè®¡ ----------
    total_decode_time = sum(decode_times)
    avg_decode_time = total_decode_time / len(decode_times) if decode_times else 0
    total_new_tokens = len(generated)
    total_generate_time = prefill_time + total_decode_time
    tokens_per_second = total_new_tokens / total_generate_time if total_generate_time > 0 else 0
    
    print("\n" + "=" * 60)
    print("[PERFORMANCE SUMMARY]")
    print("=" * 60)
    print(f"\nğŸ“Š è€—æ—¶åˆ†è§£:")
    print(f"  Model åŠ è½½:     {load_time:.4f} s")
    print(f"  Tokenize:       {tokenize_time*1000:.4f} ms")
    print(f"  Prefill:        {prefill_time*1000:.4f} ms ({prompt_len} tokens)")
    print(f"  Decode æ€»è®¡:    {total_decode_time*1000:.4f} ms ({len(decode_times)} steps)")
    print(f"  ç”Ÿæˆæ€»è€—æ—¶:     {total_generate_time*1000:.4f} ms")
    
    print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  Prefill æ¯ token:  {prefill_time*1000/prompt_len:.4f} ms/token")
    print(f"  Decode å¹³å‡:       {avg_decode_time*1000:.4f} ms/token")
    print(f"  ç”Ÿæˆé€Ÿåº¦:          {tokens_per_second:.2f} tokens/s")
    
    print(f"\nğŸ“ Decode æ¯æ­¥è€—æ—¶:")
    for i, t in enumerate(decode_times, 1):
        print(f"  Step {i}: {t*1000:.4f} ms")
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    if avg_decode_time > 0 and prefill_time/prompt_len > 0:
        ratio = avg_decode_time / (prefill_time/prompt_len)
        print(f"  Decode æ¯æ­¥è€—æ—¶ vs Prefill æ¯ token: {ratio:.2f}x")
    print(f"  KV Cache æœ€ç»ˆå¤§å°: {k0.shape} (seq_len={k0.shape[2]})")
    
    # ---------- å›ç­”å…³é”®é—®é¢˜ ----------
    print("\n" + "=" * 60)
    print("[å›ç­”: Prefill vs Decode è°æ›´æ…¢?]")
    print("=" * 60)
    print(f"""
Prefill é˜¶æ®µ:
  - å¤„ç† {prompt_len} ä¸ª tokenï¼Œè€—æ—¶ {prefill_time*1000:.2f} ms
  - å¹³å‡æ¯ token: {prefill_time*1000/prompt_len:.2f} ms
  - ç‰¹ç‚¹: ä¸€æ¬¡æ€§è®¡ç®—ï¼Œå¯å¹¶è¡Œï¼Œè®¡ç®—å¯†é›†å‹

Decode é˜¶æ®µ:
  - ç”Ÿæˆ {len(decode_times)} ä¸ª tokenï¼Œè€—æ—¶ {total_decode_time*1000:.2f} ms
  - å¹³å‡æ¯ token: {avg_decode_time*1000:.2f} ms
  - ç‰¹ç‚¹: ä¸²è¡Œæ‰§è¡Œï¼Œä¾èµ– KV Cacheï¼Œå†…å­˜å¸¦å®½å¯†é›†å‹

ç»“è®º: 
  - å•ä¸ª token æ¥çœ‹ï¼ŒDecode é€šå¸¸æ¯” Prefill æ…¢ï¼ˆéœ€è¦è¯»å– KV Cacheï¼‰
  - ä½† Prefill å¤„ç†å¤šä¸ª tokenï¼Œæ€»è€—æ—¶å¯èƒ½æ›´é•¿
  - é•¿ prompt åœºæ™¯ï¼šPrefill æ˜¯ç“¶é¢ˆ
  - é•¿ç”Ÿæˆåœºæ™¯ï¼šDecode æ˜¯ç“¶é¢ˆ
""")

if __name__ == "__main__":
    main()
