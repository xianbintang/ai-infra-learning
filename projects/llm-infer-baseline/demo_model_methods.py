"""æ¼”ç¤ºæ¨¡å‹çš„é‡è¦æ–¹æ³•ï¼ˆé™¤äº† generate ä¹‹å¤–ï¼‰ã€‚"""
from __future__ import annotations

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from config import MODEL_NAME, DEVICE, DTYPE


def demo_forward_method() -> None:
    """æ¼”ç¤º forward() / __call__() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ” æ¼”ç¤º 1: forward() / __call__() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å‡†å¤‡è¾“å…¥
    text = "Hello world"
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    
    print(f"\n1. è¾“å…¥æ–‡æœ¬: {text}")
    print(f"   è¾“å…¥ Token IDs: {inputs['input_ids']}")
    
    # æ–¹å¼ 1: ç›´æ¥è°ƒç”¨ï¼ˆæ¨èï¼‰
    print(f"\n2. ä½¿ç”¨ model() ç›´æ¥è°ƒç”¨ï¼ˆæ¨èï¼‰:")
    with torch.no_grad():
        outputs = model(
            input_ids=inputs["input_ids"],
            use_cache=True,
        )
    
    logits = outputs.logits
    past_key_values = outputs.past_key_values
    
    print(f"   logits shape: {logits.shape}")
    print(f"   logits å«ä¹‰: [batch_size={logits.shape[0]}, seq_len={logits.shape[1]}, vocab_size={logits.shape[2]}]")
    print(f"   past_key_values: {type(past_key_values)} (KV cache)")
    
    # æ–¹å¼ 2: æ˜¾å¼è°ƒç”¨ forward
    print(f"\n3. ä½¿ç”¨ model.forward() æ˜¾å¼è°ƒç”¨:")
    with torch.no_grad():
        outputs2 = model.forward(
            input_ids=inputs["input_ids"],
            use_cache=True,
        )
    
    print(f"   ä¸¤ç§æ–¹å¼ç»“æœç›¸åŒ: {torch.equal(logits, outputs2.logits)}")
    
    # è·å–ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡
    print(f"\n4. è·å–ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ:")
    next_token_logits = logits[:, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„ logits
    next_token_probs = torch.softmax(next_token_logits, dim=-1)
    top_k_probs, top_k_indices = torch.topk(next_token_probs, k=5, dim=-1)
    
    print(f"   ä¸‹ä¸€ä¸ª token çš„ top-5 é¢„æµ‹:")
    for i, (prob, idx) in enumerate(zip(top_k_probs[0], top_k_indices[0])):
        token = tokenizer.decode([idx.item()])
        print(f"     {i+1}. Token ID {idx.item():5d} ({prob.item():.4f}): '{token}'")


def demo_eval_train_methods() -> None:
    """æ¼”ç¤º eval() å’Œ train() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ¯ æ¼”ç¤º 2: eval() å’Œ train() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    
    print("\n1. é»˜è®¤æ¨¡å¼:")
    print(f"   è®­ç»ƒæ¨¡å¼: {model.training}")
    
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    print("\n2. è°ƒç”¨ model.eval():")
    model.eval()
    print(f"   è®­ç»ƒæ¨¡å¼: {model.training}")
    print(f"   ğŸ’¡ å·²ç¦ç”¨ Dropout å’Œ BatchNorm æ›´æ–°")
    
    # æ£€æŸ¥ Dropout å±‚
    dropout_modules = [m for m in model.modules() if isinstance(m, torch.nn.Dropout)]
    if dropout_modules:
        print(f"   Dropout æ¨¡å—æ•°é‡: {len(dropout_modules)}")
        print(f"   ç¬¬ä¸€ä¸ª Dropout çš„è®­ç»ƒæ¨¡å¼: {dropout_modules[0].training}")
    
    # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    print("\n3. è°ƒç”¨ model.train():")
    model.train()
    print(f"   è®­ç»ƒæ¨¡å¼: {model.training}")
    print(f"   ğŸ’¡ å·²å¯ç”¨ Dropout å’Œ BatchNorm æ›´æ–°")
    
    if dropout_modules:
        print(f"   ç¬¬ä¸€ä¸ª Dropout çš„è®­ç»ƒæ¨¡å¼: {dropout_modules[0].training}")
    
    # åˆ‡æ¢å›è¯„ä¼°æ¨¡å¼ï¼ˆæ¨ç†éœ€è¦ï¼‰
    model.eval()
    print("\n4. æ¨ç†å‰å¿…é¡»åˆ‡æ¢å› eval() æ¨¡å¼")


def demo_parameters_methods() -> None:
    """æ¼”ç¤º parameters() å’Œ named_parameters() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¼”ç¤º 3: parameters() å’Œ named_parameters() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    # ä½¿ç”¨ parameters()
    print("\n1. ä½¿ç”¨ parameters() ç»Ÿè®¡å‚æ•°:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # ä½¿ç”¨ named_parameters()
    print("\n2. ä½¿ç”¨ named_parameters() æŸ¥çœ‹å‚æ•°è¯¦æƒ…ï¼ˆå‰ 5 ä¸ªï¼‰:")
    for i, (name, param) in enumerate(model.named_parameters()):
        if i >= 5:
            break
        print(f"   {name}:")
        print(f"     - Shape: {param.shape}")
        print(f"     - å‚æ•°æ•°é‡: {param.numel():,}")
        print(f"     - éœ€è¦æ¢¯åº¦: {param.requires_grad}")
        print(f"     - è®¾å¤‡: {param.device}")
        print(f"     - æ•°æ®ç±»å‹: {param.dtype}")
    
    # ç»Ÿè®¡å„å±‚å‚æ•°
    print("\n3. æŒ‰å±‚ç»Ÿè®¡å‚æ•°:")
    layer_params = {}
    for name, param in model.named_parameters():
        layer_type = name.split('.')[0] if '.' in name else name
        if layer_type not in layer_params:
            layer_params[layer_type] = 0
        layer_params[layer_type] += param.numel()
    
    for layer, count in sorted(layer_params.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {layer}: {count:,} å‚æ•°")


def demo_state_dict_methods() -> None:
    """æ¼”ç¤º state_dict() å’Œ load_state_dict() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ æ¼”ç¤º 4: state_dict() å’Œ load_state_dict() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    # è·å–çŠ¶æ€å­—å…¸
    print("\n1. è·å– state_dict():")
    state_dict = model.state_dict()
    print(f"   çŠ¶æ€å­—å…¸é”®çš„æ•°é‡: {len(state_dict)}")
    print(f"   å‰ 5 ä¸ªé”®:")
    for i, key in enumerate(list(state_dict.keys())[:5]):
        print(f"     {i+1}. {key}: {state_dict[key].shape}")
    
    # ä¿å­˜çŠ¶æ€å­—å…¸ï¼ˆç¤ºä¾‹ï¼Œä¸å®é™…ä¿å­˜ï¼‰
    print("\n2. ä¿å­˜çŠ¶æ€å­—å…¸ï¼ˆç¤ºä¾‹ï¼‰:")
    print("   torch.save(state_dict, 'model.pth')")
    print("   ğŸ’¡ å¯ä»¥ä¿å­˜æ¨¡å‹çš„æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒº")
    
    # åŠ è½½çŠ¶æ€å­—å…¸ï¼ˆç¤ºä¾‹ï¼‰
    print("\n3. åŠ è½½çŠ¶æ€å­—å…¸ï¼ˆç¤ºä¾‹ï¼‰:")
    print("   state_dict = torch.load('model.pth')")
    print("   model.load_state_dict(state_dict)")
    print("   ğŸ’¡ å¯ä»¥æ¢å¤æ¨¡å‹çš„å‚æ•°å€¼")


def demo_to_method() -> None:
    """æ¼”ç¤º to() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸšš æ¼”ç¤º 5: to() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    
    print("\n1. åˆå§‹è®¾å¤‡:")
    first_param = next(model.parameters())
    print(f"   ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡: {first_param.device}")
    
    # è¿ç§»åˆ°è®¾å¤‡
    print(f"\n2. è¿ç§»åˆ° {DEVICE}:")
    model = model.to(DEVICE)
    first_param = next(model.parameters())
    print(f"   ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡: {first_param.device}")
    print(f"   æ•°æ®ç±»å‹: {first_param.dtype}")
    
    # éªŒè¯æ‰€æœ‰å‚æ•°éƒ½åœ¨åŒä¸€è®¾å¤‡
    devices = set(str(p.device) for p in model.parameters())
    print(f"\n3. æ‰€æœ‰å‚æ•°çš„è®¾å¤‡: {devices}")
    print(f"   ğŸ’¡ æ‰€æœ‰å‚æ•°éƒ½åœ¨åŒä¸€è®¾å¤‡: {len(devices) == 1}")


def demo_modules_methods() -> None:
    """æ¼”ç¤º modules() å’Œ named_modules() æ–¹æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ§© æ¼”ç¤º 6: modules() å’Œ named_modules() æ–¹æ³•")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    # ä½¿ç”¨ modules()
    print("\n1. ä½¿ç”¨ modules() ç»Ÿè®¡æ¨¡å—ç±»å‹:")
    module_types = {}
    for module in model.modules():
        module_type = type(module).__name__
        module_types[module_type] = module_types.get(module_type, 0) + 1
    
    print(f"   æ¨¡å—ç±»å‹ç»Ÿè®¡ï¼ˆå‰ 5 ä¸ªï¼‰:")
    for module_type, count in sorted(module_types.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"     {module_type}: {count} ä¸ª")
    
    # ä½¿ç”¨ named_modules()
    print("\n2. ä½¿ç”¨ named_modules() æŸ¥çœ‹æ¨¡å—ç»“æ„ï¼ˆå‰ 5 ä¸ªï¼‰:")
    for i, (name, module) in enumerate(model.named_modules()):
        if i >= 5:
            break
        print(f"   {name}: {type(module).__name__}")


def demo_forward_vs_generate() -> None:
    """å¯¹æ¯” forward() å’Œ generate() çš„åŒºåˆ«ã€‚"""
    print("\n" + "=" * 60)
    print("âš–ï¸  æ¼”ç¤º 7: forward() vs generate() å¯¹æ¯”")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    text = "Hello"
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    
    print(f"\nè¾“å…¥: {text}")
    
    # ä½¿ç”¨ forward()
    print("\n1. ä½¿ç”¨ forward() (å•æ¬¡å‰å‘ä¼ æ’­):")
    with torch.no_grad():
        outputs = model(input_ids=inputs["input_ids"], use_cache=True)
    
    logits = outputs.logits
    next_token_logits = logits[:, -1, :]
    next_token_id = torch.argmax(next_token_logits, dim=-1)
    
    print(f"   è¿”å›: logits shape {logits.shape}")
    print(f"   ä¸‹ä¸€ä¸ª token ID: {next_token_id.item()}")
    print(f"   ä¸‹ä¸€ä¸ª token: '{tokenizer.decode([next_token_id.item()])}'")
    print(f"   ğŸ’¡ åªæ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œéœ€è¦æ‰‹åŠ¨å®ç°ç”Ÿæˆå¾ªç¯")
    
    # ä½¿ç”¨ generate()
    print("\n2. ä½¿ç”¨ generate() (è‡ªåŠ¨ç”Ÿæˆ):")
    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_new_tokens=5,
            do_sample=False,
        )
    
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    print(f"   è¿”å›: å®Œæ•´ token åºåˆ— shape {generated.shape}")
    print(f"   ç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
    print(f"   ğŸ’¡ è‡ªåŠ¨å®Œæˆæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ï¼ˆprefill + decode loopï¼‰")
    
    print("\n3. å¯¹æ¯”:")
    print("   forward():")
    print("     - å•æ¬¡å‰å‘ä¼ æ’­")
    print("     - è¿”å› logits")
    print("     - éœ€è¦æ‰‹åŠ¨å®ç°ç”Ÿæˆå¾ªç¯")
    print("     - æ›´ç²¾ç»†çš„æ§åˆ¶")
    print("   generate():")
    print("     - è‡ªåŠ¨å®Œæˆç”Ÿæˆ")
    print("     - è¿”å›å®Œæ•´åºåˆ—")
    print("     - å†…éƒ¨å·²ä¼˜åŒ–")
    print("     - æ›´å¿«é€Ÿå¼€å‘")


def demo_manual_generation_loop() -> None:
    """æ¼”ç¤ºä½¿ç”¨ forward() å®ç°æ‰‹åŠ¨ç”Ÿæˆå¾ªç¯ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æ¼”ç¤º 8: ä½¿ç”¨ forward() å®ç°æ‰‹åŠ¨ç”Ÿæˆå¾ªç¯")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    text = "Hello"
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    
    print(f"\nè¾“å…¥: {text}")
    print(f"å¼€å§‹æ‰‹åŠ¨ç”Ÿæˆå¾ªç¯...\n")
    
    # Prefill é˜¶æ®µ
    print("1. Prefill é˜¶æ®µï¼ˆå¤„ç† promptï¼‰:")
    with torch.no_grad():
        outputs = model(input_ids=inputs["input_ids"], use_cache=True)
    
    past_key_values = outputs.past_key_values
    next_token_logits = outputs.logits[:, -1, :]
    print(f"   è·å¾— KV cache å’Œä¸‹ä¸€ä¸ª token çš„ logits")
    
    # Decode å¾ªç¯
    print("\n2. Decode å¾ªç¯ï¼ˆç”Ÿæˆæ–° tokensï¼‰:")
    generated_ids = []
    max_new_tokens = 5
    
    for step in range(max_new_tokens):
        # é€‰æ‹©ä¸‹ä¸€ä¸ª token
        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        generated_ids.append(next_token_id)
        
        token = tokenizer.decode([next_token_id.item()])
        print(f"   æ­¥éª¤ {step+1}: é€‰æ‹© token ID {next_token_id.item()} ('{token}')")
        
        # å¦‚æœé‡åˆ° EOSï¼Œåœæ­¢
        if next_token_id.item() == tokenizer.eos_token_id:
            print(f"   é‡åˆ° EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
            break
        
        # ä¸‹ä¸€æ­¥å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ KV cacheï¼‰
        with torch.no_grad():
            outputs = model(
                input_ids=next_token_id,
                past_key_values=past_key_values,
                use_cache=True,
            )
        past_key_values = outputs.past_key_values
        next_token_logits = outputs.logits[:, -1, :]
    
    # è§£ç ç»“æœ
    if generated_ids:
        all_generated = torch.cat(generated_ids, dim=1)
        full_sequence = torch.cat([inputs["input_ids"], all_generated], dim=1)
        generated_text = tokenizer.decode(full_sequence[0], skip_special_tokens=True)
        
        print(f"\n3. ç”Ÿæˆç»“æœ:")
        print(f"   ç”Ÿæˆçš„ token IDs: {[id.item() for id in generated_ids]}")
        print(f"   å®Œæ•´æ–‡æœ¬: '{generated_text}'")
        print(f"   ğŸ’¡ ä½¿ç”¨ forward() + KV cache å®ç°é«˜æ•ˆç”Ÿæˆ")


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸ”¬ æ¨¡å‹é‡è¦æ–¹æ³•æ¼”ç¤ºï¼ˆé™¤äº† generate ä¹‹å¤–ï¼‰")
    print("=" * 60)
    print(f"\næ¨¡å‹: {MODEL_NAME}")
    print(f"è®¾å¤‡: {DEVICE}")
    
    # æ¼”ç¤º 1: forward æ–¹æ³•
    demo_forward_method()
    
    # æ¼”ç¤º 2: eval/train æ–¹æ³•
    demo_eval_train_methods()
    
    # æ¼”ç¤º 3: parameters æ–¹æ³•
    demo_parameters_methods()
    
    # æ¼”ç¤º 4: state_dict æ–¹æ³•
    demo_state_dict_methods()
    
    # æ¼”ç¤º 5: to æ–¹æ³•
    demo_to_method()
    
    # æ¼”ç¤º 6: modules æ–¹æ³•
    demo_modules_methods()
    
    # æ¼”ç¤º 7: forward vs generate
    demo_forward_vs_generate()
    
    # æ¼”ç¤º 8: æ‰‹åŠ¨ç”Ÿæˆå¾ªç¯
    demo_manual_generation_loop()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 60)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("   1. forward() ç”¨äºå•æ¬¡å‰å‘ä¼ æ’­ï¼Œéœ€è¦æ‰‹åŠ¨å®ç°ç”Ÿæˆå¾ªç¯")
    print("   2. generate() è‡ªåŠ¨å®Œæˆæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹")
    print("   3. eval() æ¨ç†å‰å¿…é¡»è°ƒç”¨ï¼Œç¦ç”¨è®­ç»ƒæ—¶çš„éšæœºæ€§")
    print("   4. to(device) ç”¨äºè®¾å¤‡è¿ç§»")
    print("   5. parameters() å’Œ named_parameters() ç”¨äºå‚æ•°è®¿é—®")
    print("   6. state_dict() ç”¨äºæ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("   7. ç†è§£è¿™äº›æ–¹æ³•æœ‰åŠ©äºç²¾ç»†æ§åˆ¶æ¨¡å‹è¡Œä¸º")


if __name__ == "__main__":
    main()


