"""è¯¦ç»†è§‚å¯Ÿ tokenizer å’Œ model åŠ è½½è¿‡ç¨‹çš„è„šæœ¬ã€‚"""
from __future__ import annotations

import time
import os
from pathlib import Path

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from config import MODEL_NAME, DEVICE, DTYPE, describe_environment


def get_cache_dir() -> str:
    """è·å– Hugging Face ç¼“å­˜ç›®å½•ã€‚"""
    cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
    return os.path.join(cache_home, "hub")


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°ã€‚"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def get_model_cache_size(model_name: str) -> int:
    """è®¡ç®—æ¨¡å‹ç¼“å­˜ç›®å½•çš„æ€»å¤§å°ã€‚"""
    cache_dir = get_cache_dir()
    model_dir = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
    
    if not os.path.exists(model_dir):
        return 0
    
    total_size = 0
    for root, dirs, files in os.walk(model_dir):
        for file in files:
            file_path = os.path.join(root, file)
            try:
                total_size += os.path.getsize(file_path)
            except OSError:
                pass
    return total_size


def observe_tokenizer_loading(model_name: str) -> None:
    """è§‚å¯Ÿ tokenizer åŠ è½½è¿‡ç¨‹ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ”¤ TOKENIZER åŠ è½½è¿‡ç¨‹è§‚å¯Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¼“å­˜
    cache_dir = get_cache_dir()
    print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    
    cache_size_before = get_model_cache_size(model_name)
    if cache_size_before > 0:
        print(f"ğŸ“¦ ç¼“å­˜å¤§å°ï¼ˆåŠ è½½å‰ï¼‰: {format_size(cache_size_before)}")
        print("âœ… Tokenizer æ–‡ä»¶å·²ç¼“å­˜ï¼Œå°†ç›´æ¥ä»ç¼“å­˜åŠ è½½")
    else:
        print("âŒ Tokenizer æ–‡ä»¶æœªç¼“å­˜ï¼Œéœ€è¦ä»ç½‘ç»œä¸‹è½½")
    
    # åŠ è½½ tokenizer
    print(f"\nâ³ å¼€å§‹åŠ è½½ tokenizer: {model_name}")
    start_time = time.perf_counter()
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    load_time = time.perf_counter() - start_time
    
    print(f"âœ… Tokenizer åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.3f} ç§’")
    
    # æ£€æŸ¥ç¼“å­˜å˜åŒ–
    cache_size_after = get_model_cache_size(model_name)
    if cache_size_after > cache_size_before:
        downloaded = cache_size_after - cache_size_before
        print(f"ğŸ“¥ ä¸‹è½½å¤§å°: {format_size(downloaded)}")
    
    # æ˜¾ç¤º tokenizer ä¿¡æ¯
    print(f"\nğŸ“Š Tokenizer ä¿¡æ¯:")
    print(f"  - ç±»å‹: {type(tokenizer).__name__}")
    print(f"  - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
    print(f"  - æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
    print(f"  - BOS token: {tokenizer.bos_token}")
    print(f"  - EOS token: {tokenizer.eos_token}")
    print(f"  - PAD token: {tokenizer.pad_token}")
    
    # æµ‹è¯•ç¼–ç /è§£ç 
    test_text = "Hello, world!"
    print(f"\nğŸ§ª æµ‹è¯•ç¼–ç /è§£ç :")
    print(f"  åŸæ–‡: {test_text}")
    encoded = tokenizer.encode(test_text)
    print(f"  ç¼–ç : {encoded}")
    decoded = tokenizer.decode(encoded)
    print(f"  è§£ç : {decoded}")
    
    return tokenizer


def observe_model_loading(model_name: str, dtype: torch.dtype) -> None:
    """è§‚å¯Ÿ model åŠ è½½è¿‡ç¨‹ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ¤– MODEL åŠ è½½è¿‡ç¨‹è§‚å¯Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¼“å­˜
    cache_size_before = get_model_cache_size(model_name)
    if cache_size_before > 0:
        print(f"ğŸ“¦ ç¼“å­˜å¤§å°ï¼ˆåŠ è½½å‰ï¼‰: {format_size(cache_size_before)}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nâ³ å¼€å§‹åŠ è½½ model: {model_name}")
    print(f"  - è®¾å¤‡: {DEVICE}")
    print(f"  - æ•°æ®ç±»å‹: {dtype}")
    
    # é˜¶æ®µ 1: ä¸‹è½½å’Œè¯»å–é…ç½®
    print("\nğŸ“‹ é˜¶æ®µ 1: è¯»å–é…ç½®å’Œæ„å»ºæ¶æ„...")
    start_time = time.perf_counter()
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        dtype=dtype,
        torch_dtype=dtype,
    )
    
    config_time = time.perf_counter() - start_time
    print(f"âœ… é…ç½®è¯»å–å’Œæ¶æ„æ„å»ºå®Œæˆï¼Œè€—æ—¶: {config_time:.3f} ç§’")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print(f"\nğŸ“Š Model ä¿¡æ¯:")
    print(f"  - ç±»å‹: {type(model).__name__}")
    print(f"  - è®¾å¤‡: {next(model.parameters()).device}")
    print(f"  - æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # ä¼°ç®—æ˜¾å­˜å ç”¨
    if DEVICE == "cuda":
        param_size_mb = total_params * 2 / (1024 ** 2) if dtype == torch.float16 else total_params * 4 / (1024 ** 2)
        print(f"  - ä¼°ç®—æ˜¾å­˜å ç”¨: {param_size_mb:.2f} MB")
    
    # é˜¶æ®µ 2: è®¾å¤‡è¿ç§»
    print(f"\nğŸšš é˜¶æ®µ 2: è¿ç§»åˆ°è®¾å¤‡ {DEVICE}...")
    start_time = time.perf_counter()
    
    model = model.to(DEVICE)
    
    migrate_time = time.perf_counter() - start_time
    print(f"âœ… è®¾å¤‡è¿ç§»å®Œæˆï¼Œè€—æ—¶: {migrate_time:.3f} ç§’")
    print(f"  - å½“å‰è®¾å¤‡: {next(model.parameters()).device}")
    
    # é˜¶æ®µ 3: åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    print(f"\nğŸ¯ é˜¶æ®µ 3: åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼...")
    start_time = time.perf_counter()
    
    model.eval()
    
    eval_time = time.perf_counter() - start_time
    print(f"âœ… è¯„ä¼°æ¨¡å¼è®¾ç½®å®Œæˆï¼Œè€—æ—¶: {eval_time:.6f} ç§’")
    
    # æ£€æŸ¥ç¼“å­˜å˜åŒ–
    cache_size_after = get_model_cache_size(model_name)
    if cache_size_after > cache_size_before:
        downloaded = cache_size_after - cache_size_before
        print(f"\nğŸ“¥ ä¸‹è½½å¤§å°: {format_size(downloaded)}")
    
    return model


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸ” Tokenizer å’Œ Model åŠ è½½è¿‡ç¨‹è¯¦ç»†è§‚å¯Ÿ")
    print("=" * 60)
    print(f"\nç¯å¢ƒä¿¡æ¯: {describe_environment()}")
    print(f"æ¨¡å‹åç§°: {MODEL_NAME}")
    
    # è§‚å¯Ÿ tokenizer åŠ è½½
    tokenizer = observe_tokenizer_loading(MODEL_NAME)
    
    # è§‚å¯Ÿ model åŠ è½½
    model = observe_model_loading(MODEL_NAME, DTYPE)
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“ æ€»ç»“")
    print("=" * 60)
    print("\nâœ… Tokenizer å’Œ Model éƒ½å·²æˆåŠŸåŠ è½½ï¼")
    print("\nğŸ’¡ æç¤º:")
    print("  - é¦–æ¬¡è¿è¡Œéœ€è¦ä»ç½‘ç»œä¸‹è½½æ–‡ä»¶ï¼Œè€—æ—¶è¾ƒé•¿")
    print("  - åç»­è¿è¡Œä¼šç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œé€Ÿåº¦æ›´å¿«")
    print("  - ç¼“å­˜ä½ç½®:", get_cache_dir())
    print("  - å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ HF_HOME æ›´æ”¹ç¼“å­˜ä½ç½®")


if __name__ == "__main__":
    main()

