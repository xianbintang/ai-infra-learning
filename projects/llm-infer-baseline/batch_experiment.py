"""
Day 2: Batching å®éªŒè„šæœ¬

å®éªŒç›®æ ‡:
1. å¯¹æ¯” batch_size = 1, 4, 8 çš„æ€§èƒ½å·®å¼‚
2. æµ‹é‡ Latency vs Throughput çš„ trade-off
3. å½¢æˆ"ç‰ºç‰²è°æ¢è°"çš„å·¥ç¨‹ç›´è§‰
"""

import time
from typing import List, Dict, Any
import torch

from config import MODEL_NAME, DEVICE, describe_environment
from model import load_model
from tokenizer import load_tokenizer
from batcher import DynamicBatcher, run_batch_experiment


# ============================================================
# å®éªŒé…ç½®
# ============================================================

# æµ‹è¯• prompt æ± 
TEST_PROMPTS = [
    "Q: What is artificial intelligence?\nA:",
    "Q: Explain machine learning in simple terms.\nA:",
    "Q: What is deep learning?\nA:",
    "Q: How does natural language processing work?\nA:",
    "Q: What is a neural network?\nA:",
    "Q: Explain the transformer architecture.\nA:",
    "Q: What is GPU computing?\nA:",
    "Q: How does batch processing improve performance?\nA:",
    "Q: What is model inference?\nA:",
    "Q: Explain the concept of latency.\nA:",
    "Q: What is throughput in computing?\nA:",
    "Q: How does caching work?\nA:",
    "Q: What is parallel computing?\nA:",
    "Q: Explain CPU vs GPU.\nA:",
    "Q: What is memory bandwidth?\nA:",
    "Q: How does attention mechanism work?\nA:",
]

# å®éªŒå‚æ•°
BATCH_SIZES = [1, 4, 8]
WAIT_WINDOWS = [0, 10]  # ms
MAX_NEW_TOKENS = 32
NUM_REQUESTS = 16  # æ€»è¯·æ±‚æ•°


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


def run_all_experiments() -> List[Dict[str, Any]]:
    """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
    
    print_header("Day 2: Batching å®éªŒ")
    print(f"\nğŸ“‹ å®éªŒé…ç½®:")
    print(f"  - æ¨¡å‹: {MODEL_NAME}")
    print(f"  - è®¾å¤‡: {DEVICE}")
    print(f"  - Batch sizes: {BATCH_SIZES}")
    print(f"  - Wait windows: {WAIT_WINDOWS} ms")
    print(f"  - Max new tokens: {MAX_NEW_TOKENS}")
    print(f"  - æ€»è¯·æ±‚æ•°: {NUM_REQUESTS}")
    print(f"  - ç¯å¢ƒ: {describe_environment()}")

    # åŠ è½½æ¨¡å‹
    print_header("åŠ è½½æ¨¡å‹")
    load_start = time.perf_counter()
    model = load_model(MODEL_NAME)
    tokenizer = load_tokenizer(MODEL_NAME)
    load_time = time.perf_counter() - load_start
    print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {load_time:.2f} s")

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    prompts = (TEST_PROMPTS * ((NUM_REQUESTS // len(TEST_PROMPTS)) + 1))[:NUM_REQUESTS]
    print(f"\nå‡†å¤‡äº† {len(prompts)} ä¸ªæµ‹è¯•è¯·æ±‚")

    # è¿è¡Œå®éªŒ
    all_results = []

    print_header("å¼€å§‹å®éªŒ")

    for batch_size in BATCH_SIZES:
        for wait_window in WAIT_WINDOWS:
            exp_name = f"batch={batch_size}, wait={wait_window}ms"
            print(f"\nâ–¶ è¿è¡Œå®éªŒ: {exp_name}")

            result = run_batch_experiment(
                model=model,
                tokenizer=tokenizer,
                device=DEVICE,
                prompts=prompts,
                batch_size=batch_size,
                max_new_tokens=MAX_NEW_TOKENS,
                wait_window_ms=wait_window,
            )

            all_results.append(result)

            # æ‰“å°å®éªŒç»“æœ
            print(f"  â”œâ”€ æ€»è€—æ—¶: {result['total_time_s']:.3f} s")
            print(f"  â”œâ”€ æ€» tokens: {result['total_tokens']}")
            print(f"  â”œâ”€ ååé‡: {result['throughput_tokens_s']:.2f} tokens/s")
            print(f"  â”œâ”€ å¹³å‡å»¶è¿Ÿ: {result['latency_avg_ms']:.2f} ms")
            print(f"  â”œâ”€ P95 å»¶è¿Ÿ: {result['latency_p95_ms']:.2f} ms")
            print(f"  â””â”€ å¹³å‡æ’é˜Ÿ: {result['queue_time_avg_ms']:.2f} ms")

    return all_results


def print_comparison_table(results: List[Dict[str, Any]]):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    
    print_header("å®éªŒç»“æœå¯¹æ¯”è¡¨")

    # è¡¨å¤´
    headers = ["Batch", "Wait(ms)", "Total(s)", "Tokens", "Throughput", "Avg Lat", "P95 Lat", "Queue"]
    widths = [6, 8, 8, 7, 12, 10, 10, 8]
    
    # æ‰“å°è¡¨å¤´
    header_line = " | ".join(f"{h:^{w}}" for h, w in zip(headers, widths))
    print(header_line)
    print("-" * len(header_line))

    # æ‰“å°æ•°æ®
    for r in results:
        row = [
            f"{r['batch_size']}",
            f"{r['wait_window_ms']:.0f}",
            f"{r['total_time_s']:.3f}",
            f"{r['total_tokens']}",
            f"{r['throughput_tokens_s']:.1f} t/s",
            f"{r['latency_avg_ms']:.1f} ms",
            f"{r['latency_p95_ms']:.1f} ms",
            f"{r['queue_time_avg_ms']:.1f} ms",
        ]
        print(" | ".join(f"{v:^{w}}" for v, w in zip(row, widths)))


def analyze_tradeoff(results: List[Dict[str, Any]]):
    """åˆ†æ Trade-off"""
    
    print_header("Trade-off åˆ†æ")

    # æ‰¾å‡ºå…³é”®æ•°æ®ç‚¹
    batch_1 = [r for r in results if r['batch_size'] == 1][0]
    batch_4 = [r for r in results if r['batch_size'] == 4][0]
    batch_8 = [r for r in results if r['batch_size'] == 8][0]

    # è®¡ç®—å˜åŒ–
    throughput_gain_4 = (batch_4['throughput_tokens_s'] / batch_1['throughput_tokens_s'] - 1) * 100
    throughput_gain_8 = (batch_8['throughput_tokens_s'] / batch_1['throughput_tokens_s'] - 1) * 100

    latency_increase_4 = (batch_4['latency_avg_ms'] / batch_1['latency_avg_ms'] - 1) * 100
    latency_increase_8 = (batch_8['latency_avg_ms'] / batch_1['latency_avg_ms'] - 1) * 100

    print("\nğŸ“Š ç›¸å¯¹äº Batch=1 çš„å˜åŒ–:\n")

    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                    Batch Size å¯¹æ¯”                          â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚  æŒ‡æ ‡    â”‚     Batch=4      â”‚         Batch=8              â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Throughput â”‚ {throughput_gain_4:+.1f}% â”‚ {throughput_gain_8:+.1f}% â”‚")
    print(f"â”‚ Latency    â”‚ {latency_increase_4:+.1f}% â”‚ {latency_increase_8:+.1f}% â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # æ•ˆç‡æ¯”
    efficiency_4 = throughput_gain_4 / max(latency_increase_4, 0.1) if latency_increase_4 > 0 else float('inf')
    efficiency_8 = throughput_gain_8 / max(latency_increase_8, 0.1) if latency_increase_8 > 0 else float('inf')

    print("\nğŸ’¡ å…³é”®å‘ç°:")
    print(f"""
1. Throughput å˜åŒ–:
   - Batch=4: ååé‡ {"æå‡" if throughput_gain_4 > 0 else "ä¸‹é™"} {abs(throughput_gain_4):.1f}%
   - Batch=8: ååé‡ {"æå‡" if throughput_gain_8 > 0 else "ä¸‹é™"} {abs(throughput_gain_8):.1f}%

2. Latency å˜åŒ–:
   - Batch=4: å»¶è¿Ÿ {"å¢åŠ " if latency_increase_4 > 0 else "å‡å°‘"} {abs(latency_increase_4):.1f}%
   - Batch=8: å»¶è¿Ÿ {"å¢åŠ " if latency_increase_8 > 0 else "å‡å°‘"} {abs(latency_increase_8):.1f}%

3. ç‰ºç‰²è°æ¢è°?
   - Batch=4: ç”¨ {abs(latency_increase_4):.1f}% çš„å»¶è¿Ÿæ¢ {abs(throughput_gain_4):.1f}% çš„åå
   - Batch=8: ç”¨ {abs(latency_increase_8):.1f}% çš„å»¶è¿Ÿæ¢ {abs(throughput_gain_8):.1f}% çš„åå
""")

    # ä¸šåŠ¡å»ºè®®
    print("\nğŸ“ ä¸šåŠ¡åœºæ™¯å»ºè®®:")
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åœºæ™¯              â”‚ æ¨è Batch â”‚ åŸå›                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å®æ—¶å¯¹è¯ (Chat)   â”‚ 1-2        â”‚ ç”¨æˆ·ç­‰å¾…æ•æ„Ÿï¼Œä½å»¶è¿Ÿä¼˜å…ˆ        â”‚
â”‚ API æœåŠ¡          â”‚ 4          â”‚ å¹³è¡¡å»¶è¿Ÿå’Œåå                  â”‚
â”‚ æ‰¹é‡å¤„ç†          â”‚ 8+         â”‚ é«˜ååä¼˜å…ˆï¼Œå»¶è¿Ÿä¸æ•æ„Ÿ          â”‚
â”‚ ç¦»çº¿åˆ†æ          â”‚ æœ€å¤§å¯èƒ½    â”‚ æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


def generate_notes_update(results: List[Dict[str, Any]]) -> str:
    """ç”Ÿæˆ notes.md æ›´æ–°å†…å®¹"""
    
    # æ‰¾å‡ºå…³é”®æ•°æ®
    batch_1 = [r for r in results if r['batch_size'] == 1][0]
    batch_4 = [r for r in results if r['batch_size'] == 4][0]
    batch_8 = [r for r in results if r['batch_size'] == 8][0]

    content = f"""
---

## Experiment: day2-batching-tradeoff
Date: {time.strftime('%Y-%m-%d')}
Hardware: {describe_environment()}
Model: {MODEL_NAME}
Total Requests: {NUM_REQUESTS}
Max New Tokens: {MAX_NEW_TOKENS}

### å®éªŒæ•°æ®

| Batch | Wait(ms) | Total(s) | Throughput | Avg Lat | P95 Lat |
|-------|----------|----------|------------|---------|---------|
"""
    for r in results:
        content += f"| {r['batch_size']} | {r['wait_window_ms']:.0f} | {r['total_time_s']:.3f} | {r['throughput_tokens_s']:.1f} t/s | {r['latency_avg_ms']:.1f} ms | {r['latency_p95_ms']:.1f} ms |\n"

    # è®¡ç®—å˜åŒ–
    throughput_gain_4 = (batch_4['throughput_tokens_s'] / batch_1['throughput_tokens_s'] - 1) * 100
    throughput_gain_8 = (batch_8['throughput_tokens_s'] / batch_1['throughput_tokens_s'] - 1) * 100
    latency_increase_4 = (batch_4['latency_avg_ms'] / batch_1['latency_avg_ms'] - 1) * 100
    latency_increase_8 = (batch_8['latency_avg_ms'] / batch_1['latency_avg_ms'] - 1) * 100

    content += f"""
### Trade-off åˆ†æ

ç›¸å¯¹äº Batch=1:
- Batch=4: ååé‡ {throughput_gain_4:+.1f}%, å»¶è¿Ÿ {latency_increase_4:+.1f}%
- Batch=8: ååé‡ {throughput_gain_8:+.1f}%, å»¶è¿Ÿ {latency_increase_8:+.1f}%

### ç»“è®º

1. **ç‰ºç‰²è°æ¢è°**: Batching ç”¨å»¶è¿Ÿæ¢ååé‡
   - æ›´å¤§çš„ batch = æ›´é«˜çš„åå + æ›´é«˜çš„å»¶è¿Ÿ
   - è¿™æ˜¯ LLM æ¨ç†æœåŠ¡è®¾è®¡çš„æ ¸å¿ƒ trade-off

2. **ä¸šåŠ¡åœºæ™¯é€‰æ‹©**:
   - å®æ—¶å¯¹è¯: batch=1-2 (ä½å»¶è¿Ÿä¼˜å…ˆ)
   - API æœåŠ¡: batch=4 (å¹³è¡¡)
   - æ‰¹é‡å¤„ç†: batch=8+ (é«˜ååä¼˜å…ˆ)

3. **è§‚å¯Ÿ**:
   - CPU æ¨ç†ä¸‹ batching æ•ˆæœå—é™äºè®¡ç®—èƒ½åŠ›
   - GPU æ¨ç†ä¸‹ batching æ”¶ç›Šæ›´æ˜æ˜¾ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰
   - ç­‰å¾…çª—å£å¢åŠ ä¼šè¿›ä¸€æ­¥å¢åŠ å»¶è¿Ÿ
"""
    return content


def main():
    """ä¸»å‡½æ•°"""
    # è¿è¡Œå®éªŒ
    results = run_all_experiments()

    # æ‰“å°å¯¹æ¯”è¡¨
    print_comparison_table(results)

    # åˆ†æ Trade-off
    analyze_tradeoff(results)

    # ç”Ÿæˆ notes æ›´æ–°
    print_header("Notes æ›´æ–°å»ºè®®")
    notes_content = generate_notes_update(results)
    print("ä»¥ä¸‹å†…å®¹å¯è¿½åŠ åˆ° notes.md:\n")
    print(notes_content)

    print_header("å®éªŒå®Œæˆ")
    print("\nâœ… Day 2 Batching å®éªŒå®Œæˆ!")
    print("ğŸ“ è¯·å°†ä¸Šè¿°å†…å®¹æ›´æ–°åˆ° notes.md")


if __name__ == "__main__":
    main()

