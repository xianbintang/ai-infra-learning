"""æ¼”ç¤º model.to(DEVICE) çš„ä½œç”¨å’Œå½±å“ã€‚"""
from __future__ import annotations

import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from config import MODEL_NAME, DEVICE, DTYPE


def demo_device_mismatch_error() -> None:
    """æ¼”ç¤ºè®¾å¤‡ä¸åŒ¹é…æ—¶çš„é”™è¯¯ã€‚"""
    print("\n" + "=" * 60)
    print("âŒ æ¼”ç¤º 1: è®¾å¤‡ä¸åŒ¹é…é”™è¯¯")
    print("=" * 60)
    
    print("\nåœºæ™¯ï¼šæ¨¡å‹åœ¨ CPUï¼Œè¾“å…¥åœ¨ GPU")
    
    # åŠ è½½æ¨¡å‹ä½†ä¸è¿ç§»åˆ° GPU
    print(f"\n1. åŠ è½½æ¨¡å‹åˆ° CPU...")
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    print(f"   æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è¾“å…¥åœ¨ GPU
    print(f"\n2. å°†è¾“å…¥æ”¾åˆ° GPU...")
    inputs = tokenizer("Hello", return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
        print(f"   è¾“å…¥è®¾å¤‡: {inputs['input_ids'].device}")
    
    # å°è¯•æ¨ç†
    print(f"\n3. å°è¯•æ¨ç†ï¼ˆä¼šæŠ¥é”™ï¼‰...")
    try:
        model.eval()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=5)
        print("   âœ… æˆåŠŸï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰")
    except RuntimeError as e:
        print(f"   âŒ é”™è¯¯: {str(e)[:100]}...")
        print("\nğŸ’¡ åŸå› ï¼šæ¨¡å‹å‚æ•°åœ¨ CPUï¼Œè¾“å…¥åœ¨ GPUï¼Œæ— æ³•è®¡ç®—")


def demo_correct_usage() -> None:
    """æ¼”ç¤ºæ­£ç¡®çš„ç”¨æ³•ã€‚"""
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤º 2: æ­£ç¡®çš„ç”¨æ³•")
    print("=" * 60)
    
    print(f"\nåœºæ™¯ï¼šæ¨¡å‹å’Œè¾“å…¥éƒ½åœ¨ {DEVICE}")
    
    # åŠ è½½æ¨¡å‹å¹¶è¿ç§»
    print(f"\n1. åŠ è½½æ¨¡å‹å¹¶è¿ç§»åˆ° {DEVICE}...")
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    model = model.to(DEVICE)  # â† å…³é”®æ­¥éª¤
    model.eval()
    print(f"   æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è¾“å…¥ä¹Ÿåœ¨åŒä¸€è®¾å¤‡
    print(f"\n2. å°†è¾“å…¥æ”¾åˆ° {DEVICE}...")
    inputs = tokenizer("Hello", return_tensors="pt").to(DEVICE)
    print(f"   è¾“å…¥è®¾å¤‡: {inputs['input_ids'].device}")
    
    # æ¨ç†
    print(f"\n3. æ‰§è¡Œæ¨ç†...")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=5)
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"   âœ… æˆåŠŸï¼ç”Ÿæˆæ–‡æœ¬: {generated_text}")


def demo_performance_comparison() -> None:
    """æ¼”ç¤º CPU vs GPU æ€§èƒ½å¯¹æ¯”ã€‚"""
    print("\n" + "=" * 60)
    print("âš¡ æ¼”ç¤º 3: CPU vs GPU æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    prompt = "The quick brown fox"
    inputs = tokenizer(prompt, return_tensors="pt")
    max_new_tokens = 20
    
    # CPU æ¨ç†
    print("\nğŸ“Š CPU æ¨ç†æµ‹è¯•...")
    model_cpu = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
    model_cpu.eval()
    
    start = time.perf_counter()
    with torch.no_grad():
        outputs_cpu = model_cpu.generate(**inputs, max_new_tokens=max_new_tokens)
    cpu_time = time.perf_counter() - start
    
    cpu_text = tokenizer.decode(outputs_cpu[0], skip_special_tokens=True)
    print(f"   è€—æ—¶: {cpu_time:.3f} ç§’")
    print(f"   ç”Ÿæˆ: {cpu_text[:50]}...")
    
    # GPU æ¨ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        print("\nğŸ“Š GPU æ¨ç†æµ‹è¯•...")
        model_gpu = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=torch.float16)
        model_gpu = model_gpu.to("cuda")  # â† å…³é”®æ­¥éª¤
        model_gpu.eval()
        
        inputs_gpu = {k: v.to("cuda") for k, v in inputs.items()}
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = model_gpu.generate(**inputs_gpu, max_new_tokens=max_new_tokens)
        
        torch.cuda.synchronize()
        start = time.perf_counter()
        with torch.no_grad():
            outputs_gpu = model_gpu.generate(**inputs_gpu, max_new_tokens=max_new_tokens)
        torch.cuda.synchronize()
        gpu_time = time.perf_counter() - start
        
        gpu_text = tokenizer.decode(outputs_gpu[0], skip_special_tokens=True)
        print(f"   è€—æ—¶: {gpu_time:.3f} ç§’")
        print(f"   ç”Ÿæˆ: {gpu_text[:50]}...")
        
        if gpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"\nğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.1f}x")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œè·³è¿‡ GPU æµ‹è¯•")


def demo_memory_usage() -> None:
    """æ¼”ç¤ºè®¾å¤‡è¿ç§»å¯¹å†…å­˜/æ˜¾å­˜çš„å½±å“ã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ æ¼”ç¤º 4: å†…å­˜/æ˜¾å­˜ä½¿ç”¨")
    print("=" * 60)
    
    # è®¡ç®—æ¨¡å‹å¤§å°
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\næ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
    
    if DTYPE == torch.float32:
        size_mb = total_params * 4 / (1024 ** 2)
        print(f"float32 å¤§å°: {size_mb:.2f} MB")
    elif DTYPE == torch.float16:
        size_mb = total_params * 2 / (1024 ** 2)
        print(f"float16 å¤§å°: {size_mb:.2f} MB")
    
    # CPU å†…å­˜
    print(f"\nğŸ“Œ CPU å†…å­˜å ç”¨:")
    print(f"   æ¨¡å‹åŠ è½½å: ~{size_mb:.2f} MB (ç³»ç»Ÿå†…å­˜)")
    
    # GPU æ˜¾å­˜ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
    if DEVICE == "cuda" and torch.cuda.is_available():
        print(f"\nğŸ“Œ GPU æ˜¾å­˜å ç”¨:")
        print(f"   è¿ç§»å‰: 0 MB")
        
        model = model.to("cuda")
        
        torch.cuda.synchronize()
        allocated = torch.cuda.memory_allocated() / (1024 ** 2)
        reserved = torch.cuda.memory_reserved() / (1024 ** 2)
        
        print(f"   è¿ç§»å:")
        print(f"     - å·²åˆ†é…: {allocated:.2f} MB")
        print(f"     - å·²ä¿ç•™: {reserved:.2f} MB")
        print(f"   ğŸ’¡ æ˜¾å­˜å ç”¨ç•¥å¤§äºæ¨¡å‹å¤§å°ï¼ˆåŒ…å« PyTorch å¼€é”€ï¼‰")


def demo_what_happens_inside() -> None:
    """æ¼”ç¤º to(DEVICE) å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆã€‚"""
    print("\n" + "=" * 60)
    print("ğŸ” æ¼”ç¤º 5: to(DEVICE) å†…éƒ¨è¿‡ç¨‹")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=DTYPE)
    
    print("\n1. è¿ç§»å‰çš„è®¾å¤‡çŠ¶æ€:")
    devices_before = set()
    for name, param in model.named_parameters():
        devices_before.add(str(param.device))
        if len(devices_before) == 1:
            print(f"   å‚æ•° '{name}' åœ¨è®¾å¤‡: {param.device}")
            break
    
    print(f"   æ‰€æœ‰å‚æ•°éƒ½åœ¨: {list(devices_before)[0]}")
    
    if DEVICE == "cuda" and torch.cuda.is_available():
        print(f"\n2. æ‰§è¡Œ model.to('{DEVICE}')...")
        start = time.perf_counter()
        model = model.to(DEVICE)
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        print(f"   è€—æ—¶: {elapsed:.3f} ç§’")
        
        print(f"\n3. è¿ç§»åçš„è®¾å¤‡çŠ¶æ€:")
        devices_after = set()
        for name, param in model.named_parameters():
            devices_after.add(str(param.device))
            if len(devices_after) == 1:
                print(f"   å‚æ•° '{name}' åœ¨è®¾å¤‡: {param.device}")
                break
        
        print(f"   æ‰€æœ‰å‚æ•°éƒ½åœ¨: {list(devices_after)[0]}")
        
        print(f"\nğŸ’¡ å‘ç”Ÿäº†ä»€ä¹ˆ:")
        print(f"   - éå†äº†æ‰€æœ‰ {sum(1 for _ in model.parameters())} ä¸ªå‚æ•°")
        print(f"   - é€šè¿‡ PCIe æ€»çº¿å¤åˆ¶äº†æ•°æ®")
        print(f"   - åœ¨ GPU æ˜¾å­˜ä¸­åˆ†é…äº†æ–°ç©ºé—´")
        print(f"   - æ›´æ–°äº†æ‰€æœ‰å‚æ•°çš„è®¾å¤‡æŒ‡é’ˆ")


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸ”¬ model.to(DEVICE) è¯¦ç»†æ¼”ç¤º")
    print("=" * 60)
    print(f"\nç¯å¢ƒ: {DEVICE}, dtype: {DTYPE}")
    
    # æ¼”ç¤º 1: è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
    if torch.cuda.is_available():
        demo_device_mismatch_error()
    
    # æ¼”ç¤º 2: æ­£ç¡®ç”¨æ³•
    demo_correct_usage()
    
    # æ¼”ç¤º 3: æ€§èƒ½å¯¹æ¯”
    demo_performance_comparison()
    
    # æ¼”ç¤º 4: å†…å­˜ä½¿ç”¨
    demo_memory_usage()
    
    # æ¼”ç¤º 5: å†…éƒ¨è¿‡ç¨‹
    if torch.cuda.is_available():
        demo_what_happens_inside()
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 60)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("   1. model.to(DEVICE) å°†æ¨¡å‹å‚æ•°è¿ç§»åˆ°ç›®æ ‡è®¾å¤‡")
    print("   2. æ¨¡å‹å’Œè¾“å…¥å¿…é¡»åœ¨åŒä¸€è®¾å¤‡ï¼Œå¦åˆ™ä¼šæŠ¥é”™")
    print("   3. GPU æ¨ç†æ¯” CPU å¿« 10-100 å€")
    print("   4. è¿ç§»è¿‡ç¨‹ä¼šå¤åˆ¶æ•°æ®ï¼Œéœ€è¦æ—¶é—´ï¼ˆ0.5-2 ç§’ï¼‰")


if __name__ == "__main__":
    main()

